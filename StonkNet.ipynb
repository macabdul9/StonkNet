{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip install wandb pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import gc\n",
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "# traning, loggin and evaluation\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Datasets\n",
    "- A generic dataset class to create pytorch dataset\n",
    "- No **Text Cleaning** is being performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StonkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, file, root, max_len=512):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.read_csv(file) # slow but it's ok since we have only few thousand entries\n",
    "        self.max_len=max_len\n",
    "        self.root = root\n",
    "    \n",
    "    def read_file(self, file_name, stock, root):\n",
    "        # read a file from file_location\n",
    "        path = os.path.join(root, stock, file_name)\n",
    "        tweets = []\n",
    "        with open(path) as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                tweets.append(\" \".join(data['text']))\n",
    "\n",
    "        # remove the duplicate and preserve the order \n",
    "        \n",
    "        seen = set()\n",
    "        \n",
    "        tweets = [x for x in tweets if not (x in seen or seen.add(x))]\n",
    "        \n",
    "        return tweets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # get the label, stock name, date and tweets from the dataframe\n",
    "        \n",
    "        label = self.data['Trend'].iloc[idx]\n",
    "        \n",
    "        stock = self.data['Stock'].iloc[idx]\n",
    "        \n",
    "        file = self.data['Tweet'].iloc[idx]\n",
    "        \n",
    "        date = self.data['Date'].iloc[idx]\n",
    "        \n",
    "        tweets = self.read_file(file_name=file, stock=stock, root=self.root)\n",
    "        \n",
    "        input_ids = torch.empty((0, self.max_len), dtype=torch.long)\n",
    "        attention_masks = torch.empty((0, self.max_len), dtype=torch.long)\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            input_id, attention_mask = self.encoder(text=tweet)\n",
    "            \n",
    "            input_ids = torch.vstack((input_ids, input_id))\n",
    "            \n",
    "            attention_masks = torch.vstack((attention_masks, attention_mask))\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"stock\":stock,\n",
    "            \"date\":date,\n",
    "            \"tweets\":tweets,\n",
    "            \"input_ids\":input_ids,\n",
    "            \"attention_masks\":attention_masks,\n",
    "            \"label\":label\n",
    "        }\n",
    "\n",
    "    \n",
    "    \n",
    "    def encoder(self, text):\n",
    "        \n",
    "        encode = self.tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "        \n",
    "        return encode['input_ids'].squeeze(), encode['attention_mask'].squeeze()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. StonkNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StonkNet(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        super(StonkNet, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.base = AutoModel.from_pretrained(pretrained_model_name_or_path=self.config['model_name'])\n",
    "        \n",
    "        # free the RoBERTa \n",
    "        for param in self.base.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # GRU to prcess tweets for current day\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.config['hidden_size'],\n",
    "            hidden_size=self.config['hidden_size'],\n",
    "            num_layers=self.config['num_layers'],\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        \n",
    "        # classifier on top of GRU's last hidden state\n",
    "        self.classifier = nn.Sequential(*[\n",
    "            nn.Linear(in_features=2*self.config['hidden_size'], out_features=256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=128, out_features=self.config['num_classes'])\n",
    "        ])\n",
    "        \n",
    "        # initialize the hidden state as well as stock name\n",
    "        self.hx = None \n",
    "        self.previous_stock = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, input_ids, attention_masks, current_stock):\n",
    "        \n",
    "        \"\"\"\n",
    "            input_ids.shape = attention_masks.shape = [num_tweets, max_len]\n",
    "        \"\"\"\n",
    "        \n",
    "        _, pooler = self.base(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        \n",
    "#         pooler = outputs['pooler_output']\n",
    "        \n",
    "        # pooler.shape = [num_tweets, hidden_size] \n",
    "        \n",
    "        \n",
    "        # batchifying the pooler output\n",
    "        batch = pooler.unsqueeze(0)\n",
    "        \n",
    "       \n",
    "        # batch.shape = [1, num_tweets, hidden_size]\n",
    "        \n",
    "        # since data has been so we can only use tweets for same stocks\n",
    "        if self.previous_stock==current_stock:\n",
    "            _, hidden = self.gru(input=batch, hx=self.hx)\n",
    "            \n",
    "        else:\n",
    "            _, hidden = self.gru(input=batch, hx=None)\n",
    "            \n",
    "        self.hx = hidden.detach()\n",
    "        self.previous_stock = current_stock\n",
    "        \n",
    "        \n",
    "        # hidden.shape [2, batch_size, hidden_size] # make it batch first again\n",
    "        x = hidden.permute(1, 0, 2)\n",
    "        \n",
    "        x = torch.hstack((x[:, 0, :], x[:, 1, :]))\n",
    "        \n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        \n",
    "        return logits\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. PyTorch Lightning Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(LightningModel, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.model = StonkNet(config=config)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "        \n",
    "    def forward(self, input_ids, attention_masks, current_stock):\n",
    "        logits = self.model(input_ids=input_ids.squeeze(0), attention_masks=attention_masks.squeeze(0), current_stock=current_stock[0])\n",
    "        return logits\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(params=self.parameters(), lr=self.config['lr'])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = StonkDataset(tokenizer=self.tokenizer, file=self.config['train_file'], root=self.config['root'])\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.config['num_workers'])\n",
    "        return train_loader\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        labels = batch['label']\n",
    "        logits = self(input_ids=batch['input_ids'], attention_masks=batch['attention_masks'], current_stock=batch['stock'])\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        acc = accuracy_score(labels.cpu(), logits.argmax(dim=1).cpu())\n",
    "        f1 = f1_score(labels.cpu(), logits.argmax(dim=1).cpu(), average=self.config['average'])\n",
    "        \n",
    "        wandb.log({\"loss\":loss, \"accuracy\":acc, \"f1_score\":f1})\n",
    "        return {\"loss\":loss, \"accuracy\":acc, \"f1_score\":f1}\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = StonkDataset(tokenizer=self.tokenizer, file=self.config['valid_file'], root=self.config['root'])\n",
    "        valid_loader = DataLoader(dataset=valid_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.config['num_workers'])\n",
    "        return valid_loader\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        labels = batch['label']\n",
    "        logits = self(input_ids=batch['input_ids'], attention_masks=batch['attention_masks'], current_stock=batch['stock'])\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        acc = accuracy_score(labels.cpu(), logits.argmax(dim=1).cpu())\n",
    "        f1 = f1_score(labels.cpu(), logits.argmax(dim=1).cpu(), average=self.config['average'])\n",
    "        precision = precision_score(labels.cpu(), logits.argmax(dim=1).cpu(), average=self.config['average'])\n",
    "        recall = recall_score(labels.cpu(), logits.argmax(dim=1).cpu(), average=self.config['average'])\n",
    "        return {\"val_loss\":loss, \"val_accuracy\":torch.tensor([acc]), \"val_f1\":torch.tensor([f1]), \"val_precision\":torch.tensor([precision]), \"val_recall\":torch.tensor([recall])}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_accuracy'] for x in outputs]).mean()\n",
    "        avg_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n",
    "        avg_precision = torch.stack([x['val_precision'] for x in outputs]).mean()\n",
    "        avg_recall = torch.stack([x['val_recall'] for x in outputs]).mean()\n",
    "        wandb.log({\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_f1\":avg_f1, \"val_precision\":avg_precision, \"val_recall\":avg_recall})\n",
    "        return {\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_f1\":avg_f1, \"val_precision\":avg_precision, \"val_recall\":avg_recall}\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_dataset = StonkDataset(tokenizer=self.tokenizer, file=self.config['valid_file'], root=self.config['root'])\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.config['num_workers'])\n",
    "        return test_loader\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        labels = batch['label']\n",
    "        logits = self(input_ids=batch['input_ids'], attention_masks=batch['attention_masks'], current_stock=batch['stock'])\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        acc = accuracy_score(labels.cpu(), logits.argmax(dim=1).cpu())\n",
    "        f1 = f1_score(labels.cpu(), logits.argmax(dim=1).cpu(), average=self.config['average'])\n",
    "        precision = precision_score(labels.cpu(), logits.argmax(dim=1).cpu(), average=self.config['average'])\n",
    "        recall = recall_score(labels.cpu(), logits.argmax(dim=1).cpu(), average=self.config['average'])\n",
    "        return {\"test_loss\":loss, \"test_precision\":torch.tensor([precision]), \"test_recall\":torch.tensor([recall]), \"test_accuracy\":torch.tensor([acc]), \"test_f1\":torch.tensor([f1])}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['test_accuracy'] for x in outputs]).mean()\n",
    "        avg_f1 = torch.stack([x['test_f1'] for x in outputs]).mean()\n",
    "        avg_precision = torch.stack([x['test_precision'] for x in outputs]).mean()\n",
    "        avg_recall = torch.stack([x['test_recall'] for x in outputs]).mean()\n",
    "        return {\"test_loss\":avg_loss, \"test_precision\":avg_precision, \"test_recall\":avg_recall, \"test_acc\":avg_acc, \"test_f1\":avg_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "config  = {\n",
    "    \n",
    "    # data \n",
    "    \"root\":\"./stocknet-dataset/tweet/preprocessed/\",\n",
    "    \"train_file\":\"train.csv\",\n",
    "    \"valid_file\":\"valid.csv\",\n",
    "\n",
    "    \"max_len\":512,\n",
    "    \"batch_size\":1,\n",
    "    \"num_workers\":4,\n",
    "    \n",
    "    # model\n",
    "    \"model_name\":\"roberta-base\", #'distilbert-base-uncased',\n",
    "    \"hidden_size\":768,\n",
    "    \"num_classes\":2,\n",
    "    \"num_layers\":1,\n",
    "    \n",
    "    # training\n",
    "    \"save_dir\":\"./\",\n",
    "    \"project\":\"stonk-net\",\n",
    "    \"run_name\":\"test-run-2\",\n",
    "    \"lr\":1e-5,\n",
    "    \"monitor\":\"val_f1\",\n",
    "    \"min_delta\":0.001,\n",
    "    \"filepath\":\"./checkpoints/{epoch}-{val_f1:4f}\",\n",
    "    \"precision\":32,\n",
    "    \"average\":\"macro\",\n",
    "    \"epochs\":2,\n",
    "    \"device\":torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbLogger(\n",
    "    name=config['run_name'],\n",
    "    save_dir=config[\"save_dir\"],\n",
    "    project=config[\"project\"],\n",
    "    log_model=True,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=config[\"monitor\"],\n",
    "    min_delta=config[\"min_delta\"],\n",
    "    patience=5,\n",
    ")\n",
    "checkpoints = ModelCheckpoint(\n",
    "    filepath=config[\"filepath\"],\n",
    "    monitor=config[\"monitor\"],\n",
    "    save_top_k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "#     gpus=[0],\n",
    "    checkpoint_callback=checkpoints,\n",
    "    callbacks=[early_stopping],\n",
    "    default_root_dir=\"./models/\",\n",
    "    max_epochs=config[\"epochs\"],\n",
    "    precision=config[\"precision\"],\n",
    "    automatic_optimization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmacab\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.8<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">test-run-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/macab/stonk-net\" target=\"_blank\">https://wandb.ai/macab/stonk-net</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/macab/stonk-net/runs/2y4fkytj\" target=\"_blank\">https://wandb.ai/macab/stonk-net/runs/2y4fkytj</a><br/>\n",
       "                Run data is saved locally in <code>./wandb/run-20210215_055426-2y4fkytj</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | StonkNet | 132 M \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:22<00:22, 22.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/macab/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/12160 [00:19<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StonkNet(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 512]) torch.Size([1, 36, 512]) 1\n"
     ]
    }
   ],
   "source": [
    "print(batch['input_ids'].shape, batch['attention_masks'].shape, len(batch['stock']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = m.forward(input_ids=batch['input_ids'].squeeze(0), attention_masks=batch['attention_masks'].squeeze(0), current_stock=batch['stock'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
